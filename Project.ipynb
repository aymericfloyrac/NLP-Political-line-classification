{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOxisT-Xica4",
        "colab_type": "text"
      },
      "source": [
        "# NLP Project - Political line classification\n",
        "\n",
        "This project consists in classifying the political line of a short text, typically a tweet. The dabase used is the one used for [INSERT LINK]. It is exclusively made of tweets posted during the French 2017 presidential election. \n",
        "\n",
        "The objective of this project is to classify the tweets into one of the five following classes: far-right, right, center, left, far-left. \n",
        "\n",
        "We use different types of models, ranging from the most simple ones (such as logistic regression) to more sophisticated one (fine-tuning CamemBERT)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddoHJfggJhRl",
        "colab_type": "text"
      },
      "source": [
        "# Set up on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SGD4Py1JSEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJTNzNOdJcjQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "os.chdir('drive/My Drive/ENSAE/NLP')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoKeLGI_Jggi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDiqbR_gJrK0",
        "colab_type": "text"
      },
      "source": [
        "# 1. Data importation and description"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrcDDuyyJ680",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random \n",
        "\n",
        "np.seed(0)\n",
        "random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ht_y_EHJuuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('data/base_tweets_propre_sans_emoji.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UILfVt6KBFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, (ax1,ax2) = plt.subplots(1,2,figsize=(14,5))\n",
        "ax1.hist(df['couleur_politique'])\n",
        "ax1.set_title('Distribution of political line across the dataset')\n",
        "ax2.hist(df['mois'])\n",
        "ax2.set_title('Time distribution')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6eaXTyXjhtM",
        "colab_type": "text"
      },
      "source": [
        "Our dataset is approximately balanced, even if we have a slightly dominant class (Right). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L8ZHAR9Kcz3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.crosstab(df.mois, df.couleur_politique)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKMBk-TsLKT6",
        "colab_type": "text"
      },
      "source": [
        "# 2. Classification with random splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zR12Mr1LREN",
        "colab_type": "text"
      },
      "source": [
        "We take a first approach where we split the data without any time consideration (ie evenly across March, April and May)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOB11psaLQXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from utils.evaluation import evaluate,plot_confusion_matrix\n",
        "from utils.utils import random_split_dataset\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBokF1q5U3sj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xtrain,ytrain,Xval,yval,Xtest,ytest,label_map = random_split_dataset(df,validation=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uBdUz1fMMAG",
        "colab_type": "text"
      },
      "source": [
        "## 2.1 Classification without deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93w7ogjWVEXI",
        "colab_type": "text"
      },
      "source": [
        "We will evaluate a selection of models in order to have a benchmark for the deep learning methods we will use afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0Jph9E9VDZa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifiers = {'Logit':LogisticRegression(max_iter = 150, C = 1),\n",
        "               'SVM': SVC(C=10),\n",
        "               'RF': RandomForestClassifier(n_estimators=300)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NmFNaetfXRa",
        "colab_type": "text"
      },
      "source": [
        "We use the following code to evaluate the models on the validation set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEtcWgWxYt2Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name,clf in classifiers.items():\n",
        "    clf.fit(Xtrain,ytrain)\n",
        "    ypred = clf.predict(Xval)\n",
        "    print('------'+name+'------')\n",
        "    print(evaluate(yval,ypred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9jKEctefdkc",
        "colab_type": "text"
      },
      "source": [
        "We only show here, for clarity sake, the results with the best hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Yrb2Xgcfklh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for name,clf in classifiers.items():\n",
        "    print('------'+name+'------')\n",
        "    start = time.time()\n",
        "    clf.fit(Xtrain,ytrain)\n",
        "    end = time.time()\n",
        "    print(f'Training time: {:.1f}s')\n",
        "    ypred = clf.predict(Xtest)\n",
        "    print(classification_report(ytest,ypred,target_names = sorted(label_map)))\n",
        "    plot_confusion_matrix(ytest,ypred,label_map=label_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw-3HHNTVu_9",
        "colab_type": "text"
      },
      "source": [
        "## 2.2 Classification using custom neural networks "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDJLHyaBV9-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLgJrOaPV0v_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import BertTokenizer, AutoModel, AutoTokenizer, CamembertModel\n",
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader\n",
        "import pdb\n",
        "from sklearn.metrics import accuracy_score\n",
        "from utils.tweet_dataset import TweetDatasetBERT,TweetDataset,create_word_ix\n",
        "from utils.evaluation import get_predictions\n",
        "from classifiers import CamembertClassifier, RNN, train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxcrov92j7gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh-WFA5Pj_1C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dftrain,ytrain,dftest,ytest,label_map = random_split_dataset(df,validation=False,deep=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKyVndTF0O6D",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.1 Custom LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REsXl9w-0RwZ",
        "colab_type": "text"
      },
      "source": [
        "We implement a relatively shallow LSTM (2 layers), which uses no pretrained embedding. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfECLc3NYULh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_ix = create_word_ix(dftrain)\n",
        "train_set = TweetDataset(dftrain,word_ix)\n",
        "test_set = TweetDataset(dftest,word_ix)\n",
        "\n",
        "train_loader = DataLoader(train_set,batch_size = 32,num_workers = 5)\n",
        "test_loader = DataLoader(test_set,batch_size = 32,num_workers = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PYvbQkHs15m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(word_ix) + 1\n",
        "output_size = 5\n",
        "embedding_dim = 128\n",
        "hidden_dim = 100\n",
        "n_layers = 2\n",
        "model = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAYu43Ths-Jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zobJtkXd0JWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(model, 'rnn',criterion, optimizer,scheduler, train_loader, val_loader, n_epochs=10, print_every=100, gpu=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w8N_hQf0bqM",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.2 Custom Convolutional Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVgKbH7x0frh",
        "colab_type": "text"
      },
      "source": [
        "Convolutional networks are quite commonly used in the literature of NLP problems, so we implement a custom convolutional network. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJabc_CrV1Lo",
        "colab_type": "text"
      },
      "source": [
        "## 2.3 Classification using CamemBERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FV3frrrM0D9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dftrain,ytrain,dftest,ytest,label_map = random_split_dataset(df,validation=False,deep=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_HPAL84uHzr",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.1 Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52nMmbInj17L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating instances of training and validation set\n",
        "train_set = TweetDatasetBERT(df = dftrain, maxlen = 50, model_name='camembert-base')\n",
        "val_set = TweetDatasetBERT(df = dftest, maxlen = 50, model_name='camembert-base')\n",
        "\n",
        "#Creating intsances of training and validation dataloaders\n",
        "train_loader = DataLoader(train_set, batch_size = 32, num_workers = 5)\n",
        "val_loader = DataLoader(val_set, batch_size = 32, num_workers = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8ue8oQfsbs8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CamembertClassifier('camembert-base')\n",
        "model = model.to(\"cuda\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5xqf8oyspt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
        "optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
        "scheduler = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLpM5QZMsrqw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(model, 'bert',criterion, optimizer,scheduler, train_loader, val_loader, n_epochs=10, print_every=100, gpu=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQJilybFuL5b",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.2 Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqeE_qYBtYPo",
        "colab_type": "text"
      },
      "source": [
        "For the evaluation part, since we are interested in recall, precision and f1-score, we cannot simply compute an average over batches. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbv7YBD2tmlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.eval()\n",
        "yval,ypred = get_predictions(model,val_loader)\n",
        "print(classification_report(yval,ypred,target_names = sorted(label_map)))\n",
        "plot_confusion_matrix(ytest,ypred,label_map=label_map)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3yhRpU5uF74",
        "colab_type": "text"
      },
      "source": [
        "### 2.3.3 Interpretation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pegKhkj5uUJ4",
        "colab_type": "text"
      },
      "source": [
        "For this part, we want to visualize which parts of text caught the attention of the classifier. This gives us an insight about how the model works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5Hzm5fKuTdW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils.interpretation import get_last_layer_attention,display_attention,visualize_layer_attention"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6g1fSou6usAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = train_set[1]\n",
        "mean_attn,tokens,attn_data = get_last_layer_attention(model,train_set.tokenizer,sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SQTDb77u5lR",
        "colab_type": "text"
      },
      "source": [
        "We visualize the attention of the 12 attention heads of the encoder's last layer (which is the one given in input ot the classifier part of our model)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_aZ1TDtuz4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualize_layer_attention(attn_data['layer11'].cpu().detach().numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgkjBRXTvFqp",
        "colab_type": "text"
      },
      "source": [
        "Below is a more compact and more explicit visualization of these same attentions. it is obtained by averaging over attentions heads and rows, getting rid of useless tokens (start, end and padding tokens) and summing the attentions over tokens which are parts of the same word. Let's notice however that this procedure does not give us the ties between words of the sentence, but only which words are important and have strong ties with a certain number of other words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7k6K9tIu4uo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display_attention(tokens,mean_attn,train_set.tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE108FMbvYLn",
        "colab_type": "text"
      },
      "source": [
        "In most cases this points to significant words in the sentences, such as \"progress\", \"solidarity\", \"deregulation\", which may mean that some political parties use them more than others (which is intuitive). Also, when the name of the candidate is present in the sentence, it attracts the attention of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz_ZuFomLh8Y",
        "colab_type": "text"
      },
      "source": [
        "# 3. Classification with time-wise splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62YpzBkKLlg0",
        "colab_type": "text"
      },
      "source": [
        "In this second approach, we split the data time-wise: we train on March and we test on April and May. We expect the performances of our models to drop, especially those which are based on the count vectorizer, which will face many out-of-vocabulary words in the test set. The reason for that is that our tweets are highly contextual: they respond to events happening in a short time window before the tweet. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIZ8wcyALkrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}